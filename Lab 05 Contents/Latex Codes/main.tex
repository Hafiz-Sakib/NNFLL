\documentclass[a4paper,12pt]{article}

% Setting up the geometry for the document
\usepackage[margin=1in]{geometry}

% Including essential packages for mathematical typesetting and tables
\usepackage{amsmath, amssymb}
\usepackage{booktabs}
\usepackage{array}
\usepackage{graphicx} % For including images
\usepackage{caption} % For better caption control
\usepackage{siunitx} % For formatting numbers and units
\usepackage{enumitem} % For better enumeration
\usepackage{hyperref} % For hyperlinks
\usepackage{xcolor} % For colored text

% Setting up the font as requested
\usepackage{mathpazo} % Palatino font with math support

% Ensuring proper spacing and formatting
\usepackage{parskip}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}

\begin{document}

% Introduction
\section*{Introduction}
This lab report extends the previous analysis titled \textbf{Introduction to TensorFlow and Keras to Build and Train Neural Networks for Structured Data}, focusing on enhancing neural network models for the Pima Indians Diabetes Dataset and introducing convolutional neural networks (CNNs) for an image-based dataset. The objectives are twofold: first, to evaluate a new neural network model incorporating dropout and normalization techniques on the Pima Indians Diabetes Dataset to improve generalization and robustness; second, to develop and compare a custom CNN model and a pretrained ResNet model for an image dataset, analyzing their performance in terms of accuracy and loss. Early stopping is employed across all models to prevent overfitting or underfitting. This report details the dataset characteristics, preprocessing steps, model architectures, and performance analysis, including loss and accuracy curves, to provide insights into the effectiveness of these approaches for structured and image-based classification tasks.

% Section 1: New Model with Dropout and Normalization
\section*{New Model with Dropout and Normalization}
To enhance the generalization of the neural network on the Pima Indians Diabetes Dataset, a new model incorporating dropout and batch normalization is introduced. Dropout randomly deactivates a fraction of neurons during training to prevent overfitting, while batch normalization normalizes layer inputs to stabilize and accelerate training. The model architecture is similar to the previous Forward Propagation model but includes dropout layers (rate = 0.3) after each hidden layer and batch normalization before activations.

\subsection*{Dropout and Normalization Model Code}
\begin{verbatim}
# Pseudocode for Dropout and Normalization Model

# Preprocess data
Load dataset: Pima Indians Diabetes Dataset
Replace zeros with mean in [Glucose, BloodPressure, SkinThickness, Insulin, BMI]
Set features X (all except Outcome), target y (Outcome)
Split data: 80% train, 20% test
Normalize features with StandardScaler

# Define early stopping
Monitor validation loss, patience 10 epochs, restore best weights

# Define model
Sequential model:
    Layer 1: Dense(8, input_shape=(8,))
    BatchNormalization
    Activation: ReLU
    Dropout: 0.3
    Layer 2: Dense(4)
    BatchNormalization
    Activation: ReLU
    Dropout: 0.3
    Output: Dense(1, activation='sigmoid')

# Train with Adam
Compile: Adam optimizer, binary crossentropy, accuracy
Train: 100 epochs, batch size 32, early stopping
Store history (Adam)

# Train with SGD
Compile: SGD optimizer, binary crossentropy, accuracy
Train: 100 epochs, batch size 32, early stopping
Store history (SGD)
\end{verbatim}

\subsection*{Dropout and Normalization Model Performance, Loss, and Accuracy Graphs Analysis}
The model was trained with both Adam and SGD optimizers, using early stopping to halt training when validation loss ceased to improve. The inclusion of dropout and batch normalization aims to reduce overfitting compared to the baseline models while maintaining or improving performance. The performance graphs below illustrate the training and validation accuracy and loss trends for both optimizers.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{dropout_model_1.png}
    \includegraphics[width=0.9\textwidth]{dropout_model_2.png}
    \caption{Performance of Dropout and Normalization Model (Adam and SGD Optimizers) - Accuracy and Loss}
\end{figure}

% Section 2: Image Dataset and CNN Models
\section*{Image Dataset and CNN Models}
A new image dataset is introduced for binary classification (details to be specified, e.g., diabetic retinopathy images or a similar medical imaging dataset). The dataset is assumed to contain labeled images (e.g., positive/negative for a condition), preprocessed to a uniform size (e.g., 224x224 pixels) and normalized. Two models are implemented: a custom CNN and a pretrained ResNet model fine-tuned for the task. Early stopping is used to address potential underfitting or overfitting.

\subsection*{Details of the Image Dataset}
The image dataset (name and source to be specified) consists of images with binary labels. The dataset is split into 80\% training and 20\% testing sets. Preprocessing includes resizing images to 224x224 pixels, normalizing pixel values to [0,1], and applying data augmentation (e.g., rotation, flipping) to enhance generalization.

\subsection*{Custom CNN Model}
The custom CNN model is designed for the image dataset, featuring convolutional layers for feature extraction, followed by pooling, dropout, and dense layers for classification.

\subsubsection*{Custom CNN Model Code}
\begin{verbatim}
# Pseudocode for Custom CNN Model

# Preprocess data
Load image dataset
Resize images to 224x224 pixels
Normalize pixel values to [0,1]
Apply data augmentation: rotation, flip, zoom
Split data: 80% train, 20% test

# Define early stopping
Monitor validation loss, patience 10 epochs, restore best weights

# Define model
Sequential model:
    Conv2D: 32 filters, 3x3 kernel, ReLU, input_shape=(224,224,3)
    MaxPooling2D: 2x2 pool
    Conv2D: 64 filters, 3x3 kernel, ReLU
    MaxPooling2D: 2x2 pool
    Conv2D: 128 filters, 3x3 kernel, ReLU
    MaxPooling2D: 2x2 pool
    Flatten
    Dense: 128 neurons, ReLU
    Dropout: 0.5
    Output: Dense(1, activation='sigmoid')

# Train with Adam
Compile: Adam optimizer, binary crossentropy, accuracy
Train: 50 epochs, batch size 32, early stopping
Store history (Adam)

# Train with SGD
Compile: SGD optimizer, binary crossentropy, accuracy
Train: 50 epochs, batch size 32, early stopping
Store history (SGD)
\end{verbatim}

\subsubsection*{Custom CNN Model Performance, Loss, and Accuracy Graphs Analysis}
The custom CNN was trained with Adam and SGD optimizers, with early stopping to prevent overfitting. The loss and accuracy curves below show the model's convergence behavior on the image dataset.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{cnn_custom_1.png}
    \includegraphics[width=0.9\textwidth]{cnn_custom_2.png}
    \caption{Performance of Custom CNN Model (Adam and SGD Optimizers) - Accuracy and Loss}
\end{figure}

\subsection*{Pretrained ResNet Model}
A pretrained ResNet-50 model, pre-trained on ImageNet, is fine-tuned for the image dataset. The base model’s weights are frozen, and a custom classification head is added.

\subsubsection*{Pretrained ResNet Model Code}
\begin{verbatim}
# Pseudocode for Pretrained ResNet Model

# Preprocess data
Load image dataset
Resize images to 224x224 pixels
Normalize pixel values using ImageNet mean and std
Apply data augmentation: rotation, flip, zoom
Split data: 80% train, 20% test

# Define early stopping
Monitor validation loss, patience 10 epochs, restore best weights

# Define model
Load ResNet-50: pretrained on ImageNet, exclude top layers
Freeze base model layers
Add layers:
    GlobalAveragePooling2D
    Dense: 128 neurons, ReLU
    Dropout: 0.5
    Output: Dense(1, activation='sigmoid')

# Train with Adam
Compile: Adam optimizer, binary crossentropy, accuracy
Train: 50 epochs, batch size 32, early stopping
Store history (Adam)

# Train with SGD
Compile: SGD optimizer, binary crossentropy, accuracy
Train: 50 epochs, batch size 32, early stopping
Store history (SGD)
\end{verbatim}

\subsubsection*{Pretrained ResNet Model Performance, Loss, and Accuracy Graphs Analysis}
The pretrained ResNet-50 model was fine-tuned with Adam and SGD optimizers, using early stopping to ensure optimal convergence. The loss and accuracy curves below highlight the model’s performance.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{resnet_pretrained_1.png}
    \includegraphics[width=0.9\textwidth]{resnet_pretrained_2.png}
    \caption{Performance of Pretrained ResNet Model (Adam and SGD Optimizers) - Accuracy and Loss}
\end{figure}

% Section 3: Early Stopping
\section*{Early Stopping}
Early stopping was implemented using the Keras \texttt{EarlyStopping} callback, monitoring validation loss with a patience of 10 epochs and restoring the best weights. This was applied to all models (Dropout and Normalization, Custom CNN, and Pretrained ResNet) to prevent overfitting or underfitting, ensuring training stops when validation performance plateaus. The effects are visible in the loss and accuracy curves in the respective model performance subsections.

% Section 4: Optimizers
\section*{Optimizers}
Two optimizers were used for all models:
\begin{itemize}
    \item \textbf{Adam}: An adaptive learning rate optimizer combining momentum and RMSProp for efficient convergence.
    \item \textbf{SGD}: Stochastic Gradient Descent with a fixed learning rate, emphasizing traditional optimization.
\end{itemize}
Performance differences are analyzed in the loss and accuracy curves for each model.

% Section 5: Loss and Accuracy Curves
\section*{Loss and Accuracy Curves}
The loss and accuracy curves for all models (Dropout and Normalization, Custom CNN, and Pretrained ResNet) with both Adam and SGD optimizers are presented in their respective subsections. These curves illustrate the training dynamics, convergence speed, and the impact of early stopping on preventing overfitting or underfitting.

% Section 6: Findings
\section*{Findings}
The analysis of the Pima Indians Diabetes Dataset and the image dataset yielded the following insights:

\begin{itemize}
    \item \textbf{Dropout and Normalization Model}: The inclusion of dropout (0.3) and batch normalization improved generalization compared to the baseline Forward Propagation model. Early stopping ensured stable performance, with Adam converging faster than SGD. (Performance metrics to be specified based on your results.)
    
    \item \textbf{Custom CNN Model}: The custom CNN effectively extracted features from the image dataset, with early stopping preventing overfitting. Adam typically outperformed SGD in convergence speed, but SGD provided stable performance for simpler architectures. (Metrics to be specified.)
    
    \item \textbf{Pretrained ResNet Model}: Fine-tuning ResNet-50 leveraged pretrained features, yielding competitive performance with fewer training epochs. Early stopping was critical to avoid overfitting due to the model’s high capacity. (Metrics to be specified.)
    
    \item \textbf{Optimizer Performance}: Adam generally achieved faster convergence across all models, but SGD provided robust results for simpler architectures or when fine-tuning pretrained models.
    
    \item \textbf{Early Stopping}: Essential for all models to prevent overfitting (especially in high-capacity models like ResNet) and underfitting (in simpler models with SGD).
    
    \item \textbf{Dataset Considerations}: The Pima Indians Diabetes Dataset’s small size and class imbalance limited recall, while the image dataset benefited from data augmentation to enhance generalization. Future work could explore advanced augmentation or transfer learning techniques.
    
    \item \textbf{Model Recommendation}: The best model (to be determined based on your F1-score or accuracy metrics) is recommended for each task, balancing performance and computational efficiency.
\end{itemize}

\end{document}