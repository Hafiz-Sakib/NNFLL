# -*- coding: utf-8 -*-
"""lab04.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10xkl0kudIbuqXsPBs88WbIY_s9JrQfp8
"""

# Python code for Forward Propagation Model (fixed)
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import EarlyStopping

# Load dataset (your file already has headers)
file_path = "/content/diabetes.csv"
data = pd.read_csv(file_path)

# Replace zeros with NaN for specific columns
cols_to_fix = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']
for col in cols_to_fix:
    data[col] = data[col].replace(0, np.nan)
    data[col] = data[col].fillna(data[col].mean())

# Define features and target
X = data.drop('Outcome', axis=1)
y = data['Outcome']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Forward Propagation Model
model_forward = Sequential([
    Dense(8, activation='relu', input_shape=(8,)),
    Dense(4, activation='relu'),
    Dense(1, activation='sigmoid')
])

# Train with Adam optimizer
model_forward.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history_forward_adam = model_forward.fit(
    X_train, y_train, validation_data=(X_test, y_test),
    epochs=100, batch_size=32, callbacks=[early_stopping], verbose=0
)

# Train with SGD optimizer
model_forward.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])
history_forward_sgd = model_forward.fit(
    X_train, y_train, validation_data=(X_test, y_test),
    epochs=100, batch_size=32, callbacks=[early_stopping], verbose=0
)

import matplotlib.pyplot as plt

# Evaluate models on the test set
adam_loss, adam_acc = model_forward.evaluate(X_test, y_test, verbose=0)
sgd_loss, sgd_acc = model_forward.evaluate(X_test, y_test, verbose=0)

print("ðŸ”¹ Adam Optimizer:")
print(f"Test Accuracy: {adam_acc:.4f}")
print(f"Test Loss: {adam_loss:.4f}\n")

print("ðŸ”¹ SGD Optimizer:")
print(f"Test Accuracy: {sgd_acc:.4f}")
print(f"Test Loss: {sgd_loss:.4f}")

# Plot training & validation accuracy
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.plot(history_forward_adam.history['accuracy'], label='Adam Train Acc')
plt.plot(history_forward_adam.history['val_accuracy'], label='Adam Val Acc')
plt.plot(history_forward_sgd.history['accuracy'], label='SGD Train Acc')
plt.plot(history_forward_sgd.history['val_accuracy'], label='SGD Val Acc')
plt.title("Model Accuracy Comparison")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()

# Plot training & validation loss
plt.subplot(1,2,2)
plt.plot(history_forward_adam.history['loss'], label='Adam Train Loss')
plt.plot(history_forward_adam.history['val_loss'], label='Adam Val Loss')
plt.plot(history_forward_sgd.history['loss'], label='SGD Train Loss')
plt.plot(history_forward_sgd.history['val_loss'], label='SGD Val Loss')
plt.title("Model Loss Comparison")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()

plt.show()

import matplotlib.pyplot as plt

def plot_early_stopping(history, title):
    epochs = range(1, len(history.history['loss']) + 1)

    plt.figure(figsize=(12,5))

    # Plot accuracy
    plt.subplot(1,2,1)
    plt.plot(epochs, history.history['accuracy'], 'b-', label='Training Accuracy')
    plt.plot(epochs, history.history['val_accuracy'], 'r-', label='Validation Accuracy')
    plt.axvline(len(epochs), color='g', linestyle='--', label='Early Stopping Point')
    plt.title(f'{title} - Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot loss
    plt.subplot(1,2,2)
    plt.plot(epochs, history.history['loss'], 'b-', label='Training Loss')
    plt.plot(epochs, history.history['val_loss'], 'r-', label='Validation Loss')
    plt.axvline(len(epochs), color='g', linestyle='--', label='Early Stopping Point')
    plt.title(f'{title} - Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

# Plot early stopping for both optimizers
plot_early_stopping(history_forward_adam, "Adam Optimizer")
plot_early_stopping(history_forward_sgd, "SGD Optimizer")

# Python code for Backward Propagation Model
# (Assumes data preprocessing from Forward Propagation Model)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Build Backward Propagation Model (same architecture as Forward Propagation)
model_backward = Sequential([
    Dense(8, activation='relu', input_shape=(8,)),
    Dense(4, activation='relu'),
    Dense(1, activation='sigmoid')
])

# Train with Adam optimizer
model_backward.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history_backward_adam = model_backward.fit(
    X_train, y_train, validation_data=(X_test, y_test),
    epochs=100, batch_size=32, callbacks=[early_stopping], verbose=0
)

# Train with SGD optimizer
model_backward.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])
history_backward_sgd = model_backward.fit(
    X_train, y_train, validation_data=(X_test, y_test),
    epochs=100, batch_size=32, callbacks=[early_stopping], verbose=0
)

import matplotlib.pyplot as plt

# Evaluate models on the test set
back_adam_loss, back_adam_acc = model_backward.evaluate(X_test, y_test, verbose=0)
back_sgd_loss, back_sgd_acc = model_backward.evaluate(X_test, y_test, verbose=0)

print("ðŸ”¹ Backpropagation - Adam Optimizer:")
print(f"Test Accuracy: {back_adam_acc:.4f}")
print(f"Test Loss: {back_adam_loss:.4f}\n")

print("ðŸ”¹ Backpropagation - SGD Optimizer:")
print(f"Test Accuracy: {back_sgd_acc:.4f}")
print(f"Test Loss: {back_sgd_loss:.4f}")

# Plot training & validation accuracy
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.plot(history_backward_adam.history['accuracy'], label='Adam Train Acc')
plt.plot(history_backward_adam.history['val_accuracy'], label='Adam Val Acc')
plt.plot(history_backward_sgd.history['accuracy'], label='SGD Train Acc')
plt.plot(history_backward_sgd.history['val_accuracy'], label='SGD Val Acc')
plt.title("Backpropagation - Accuracy Comparison")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()

# Plot training & validation loss
plt.subplot(1,2,2)
plt.plot(history_backward_adam.history['loss'], label='Adam Train Loss')
plt.plot(history_backward_adam.history['val_loss'], label='Adam Val Loss')
plt.plot(history_backward_sgd.history['loss'], label='SGD Train Loss')
plt.plot(history_backward_sgd.history['val_loss'], label='SGD Val Loss')
plt.title("Backpropagation - Loss Comparison")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()

plt.show()

def plot_early_stopping(history, title):
    epochs = range(1, len(history.history['loss']) + 1)

    plt.figure(figsize=(12,5))

    # Accuracy plot
    plt.subplot(1,2,1)
    plt.plot(epochs, history.history['accuracy'], 'b-', label='Training Accuracy')
    plt.plot(epochs, history.history['val_accuracy'], 'r-', label='Validation Accuracy')
    plt.axvline(len(epochs), color='g', linestyle='--', label='Early Stopping Point')
    plt.title(f'{title} - Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss plot
    plt.subplot(1,2,2)
    plt.plot(epochs, history.history['loss'], 'b-', label='Training Loss')
    plt.plot(epochs, history.history['val_loss'], 'r-', label='Validation Loss')
    plt.axvline(len(epochs), color='g', linestyle='--', label='Early Stopping Point')
    plt.title(f'{title} - Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

# Plot early stopping for both backpropagation optimizers
plot_early_stopping(history_backward_adam, "Backpropagation - Adam Optimizer")
plot_early_stopping(history_backward_sgd, "Backpropagation - SGD Optimizer")

# Python code for Wider Model
# (Assumes data preprocessing from Forward Propagation Model)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Build Wider Model (increased neurons per layer: 32, 16)
model_wider = Sequential([
    Dense(32, activation='relu', input_shape=(8,)),
    Dense(16, activation='relu'),
    Dense(1, activation='sigmoid')
])

# Train with Adam optimizer
model_wider.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history_wider_adam = model_wider.fit(
    X_train, y_train, validation_data=(X_test, y_test),
    epochs=100, batch_size=32, callbacks=[early_stopping], verbose=0
)

# Train with SGD optimizer
model_wider.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])
history_wider_sgd = model_wider.fit(
    X_train, y_train, validation_data=(X_test, y_test),
    epochs=100, batch_size=32, callbacks=[early_stopping], verbose=0
)

import matplotlib.pyplot as plt

# Evaluate models on the test set
wider_adam_loss, wider_adam_acc = model_wider.evaluate(X_test, y_test, verbose=0)
wider_sgd_loss, wider_sgd_acc = model_wider.evaluate(X_test, y_test, verbose=0)

print("ðŸ”¹ Wider Model - Adam Optimizer:")
print(f"Test Accuracy: {wider_adam_acc:.4f}")
print(f"Test Loss: {wider_adam_loss:.4f}\n")

print("ðŸ”¹ Wider Model - SGD Optimizer:")
print(f"Test Accuracy: {wider_sgd_acc:.4f}")
print(f"Test Loss: {wider_sgd_loss:.4f}")

# Plot training & validation accuracy
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.plot(history_wider_adam.history['accuracy'], label='Adam Train Acc')
plt.plot(history_wider_adam.history['val_accuracy'], label='Adam Val Acc')
plt.plot(history_wider_sgd.history['accuracy'], label='SGD Train Acc')
plt.plot(history_wider_sgd.history['val_accuracy'], label='SGD Val Acc')
plt.title("Wider Model - Accuracy Comparison")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()

# Plot training & validation loss
plt.subplot(1,2,2)
plt.plot(history_wider_adam.history['loss'], label='Adam Train Loss')
plt.plot(history_wider_adam.history['val_loss'], label='Adam Val Loss')
plt.plot(history_wider_sgd.history['loss'], label='SGD Train Loss')
plt.plot(history_wider_sgd.history['val_loss'], label='SGD Val Loss')
plt.title("Wider Model - Loss Comparison")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()

plt.show()

def plot_early_stopping(history, title):
    epochs = range(1, len(history.history['loss']) + 1)

    plt.figure(figsize=(12,5))

    # Accuracy plot
    plt.subplot(1,2,1)
    plt.plot(epochs, history.history['accuracy'], 'b-', label='Training Accuracy')
    plt.plot(epochs, history.history['val_accuracy'], 'r-', label='Validation Accuracy')
    plt.axvline(len(epochs), color='g', linestyle='--', label='Early Stopping Point')
    plt.title(f'{title} - Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss plot
    plt.subplot(1,2,2)
    plt.plot(epochs, history.history['loss'], 'b-', label='Training Loss')
    plt.plot(epochs, history.history['val_loss'], 'r-', label='Validation Loss')
    plt.axvline(len(epochs), color='g', linestyle='--', label='Early Stopping Point')
    plt.title(f'{title} - Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

# Plot early stopping for both Wider Model optimizers
plot_early_stopping(history_wider_adam, "Wider Model - Adam Optimizer")
plot_early_stopping(history_wider_sgd, "Wider Model - SGD Optimizer")

# Python code for Deeper Model
# (Assumes data preprocessing from Forward Propagation Model)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Build Deeper Model (five layers: 8, 8, 6, 4, 2 neurons)
model_deeper = Sequential([
    Dense(8, activation='relu', input_shape=(8,)),
    Dense(8, activation='relu'),
    Dense(6, activation='relu'),
    Dense(4, activation='relu'),
    Dense(2, activation='relu'),
    Dense(1, activation='sigmoid')
])

# Train with Adam optimizer
model_deeper.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history_deeper_adam = model_deeper.fit(
    X_train, y_train, validation_data=(X_test, y_test),
    epochs=100, batch_size=32, callbacks=[early_stopping], verbose=0
)

# Train with SGD optimizer
model_deeper.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])
history_deeper_sgd = model_deeper.fit(
    X_train, y_train, validation_data=(X_test, y_test),
    epochs=100, batch_size=32, callbacks=[early_stopping], verbose=0
)

import matplotlib.pyplot as plt

# Evaluate models on the test set
deeper_adam_loss, deeper_adam_acc = model_deeper.evaluate(X_test, y_test, verbose=0)
deeper_sgd_loss, deeper_sgd_acc = model_deeper.evaluate(X_test, y_test, verbose=0)

print("ðŸ”¹ Deeper Model - Adam Optimizer:")
print(f"Test Accuracy: {deeper_adam_acc:.4f}")
print(f"Test Loss: {deeper_adam_loss:.4f}\n")

print("ðŸ”¹ Deeper Model - SGD Optimizer:")
print(f"Test Accuracy: {deeper_sgd_acc:.4f}")
print(f"Test Loss: {deeper_sgd_loss:.4f}")

# Plot training & validation accuracy
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.plot(history_deeper_adam.history['accuracy'], label='Adam Train Acc')
plt.plot(history_deeper_adam.history['val_accuracy'], label='Adam Val Acc')
plt.plot(history_deeper_sgd.history['accuracy'], label='SGD Train Acc')
plt.plot(history_deeper_sgd.history['val_accuracy'], label='SGD Val Acc')
plt.title("Deeper Model - Accuracy Comparison")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()

# Plot training & validation loss
plt.subplot(1,2,2)
plt.plot(history_deeper_adam.history['loss'], label='Adam Train Loss')
plt.plot(history_deeper_adam.history['val_loss'], label='Adam Val Loss')
plt.plot(history_deeper_sgd.history['loss'], label='SGD Train Loss')
plt.plot(history_deeper_sgd.history['val_loss'], label='SGD Val Loss')
plt.title("Deeper Model - Loss Comparison")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()

plt.show()

def plot_early_stopping(history, title):
    epochs = range(1, len(history.history['loss']) + 1)

    plt.figure(figsize=(12,5))

    # Accuracy plot
    plt.subplot(1,2,1)
    plt.plot(epochs, history.history['accuracy'], 'b-', label='Training Accuracy')
    plt.plot(epochs, history.history['val_accuracy'], 'r-', label='Validation Accuracy')
    plt.axvline(len(epochs), color='g', linestyle='--', label='Early Stopping Point')
    plt.title(f'{title} - Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()

    # Loss plot
    plt.subplot(1,2,2)
    plt.plot(epochs, history.history['loss'], 'b-', label='Training Loss')
    plt.plot(epochs, history.history['val_loss'], 'r-', label='Validation Loss')
    plt.axvline(len(epochs), color='g', linestyle='--', label='Early Stopping Point')
    plt.title(f'{title} - Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

# Plot early stopping for both Deeper Model optimizers
plot_early_stopping(history_deeper_adam, "Deeper Model - Adam Optimizer")
plot_early_stopping(history_deeper_sgd, "Deeper Model - SGD Optimizer")



# Plot combined training & validation accuracy
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history_forward_adam.history['val_accuracy'], label='Forward Adam Val Acc')
plt.plot(history_forward_sgd.history['val_accuracy'], label='Forward SGD Val Acc')
plt.plot(history_backward_adam.history['val_accuracy'], label='Backward Adam Val Acc')
plt.plot(history_backward_sgd.history['val_accuracy'], label='Backward SGD Val Acc')
plt.plot(history_wider_adam.history['val_accuracy'], label='Wider Adam Val Acc')
plt.plot(history_wider_sgd.history['val_accuracy'], label='Wider SGD Val Acc')
plt.plot(history_deeper_adam.history['val_accuracy'], label='Deeper Adam Val Acc')
plt.plot(history_deeper_sgd.history['val_accuracy'], label='Deeper SGD Val Acc')
plt.title("Model Validation Accuracy Comparison")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()

# Plot combined training & validation loss
plt.subplot(1, 2, 2)
plt.plot(history_forward_adam.history['val_loss'], label='Forward Adam Val Loss')
plt.plot(history_forward_sgd.history['val_loss'], label='Forward SGD Val Loss')
plt.plot(history_backward_adam.history['val_loss'], label='Backward Adam Val Loss')
plt.plot(history_backward_sgd.history['val_loss'], label='Backward SGD Val Loss')
plt.plot(history_wider_adam.history['val_loss'], label='Wider Adam Val Loss')
plt.plot(history_wider_sgd.history['val_loss'], label='Wider SGD Val Loss')
plt.plot(history_deeper_adam.history['val_loss'], label='Deeper Adam Val Loss')
plt.plot(history_deeper_sgd.history['val_loss'], label='Deeper SGD Val Loss')
plt.title("Model Validation Loss Comparison")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()

plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt

# Load dataset
data = pd.read_csv('/content/diabetes.csv')

# Replace zeros with NaN for specific columns
for col in ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']:
    data[col].replace(0, np.nan, inplace=True)
    data[col].fillna(data[col].mean(), inplace=True)

# Define features and target
X = data.drop('Outcome', axis=1)
y = data['Outcome']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Forward Propagation Model
model_forward = Sequential([
    Dense(8, activation='relu', input_shape=(8,)),
    Dense(4, activation='relu'),
    Dense(1, activation='sigmoid')
])
model_forward.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history_forward_adam = model_forward.fit(X_train, y_train, validation_data=(X_test, y_test),
                                        epochs=100, batch_size=32, callbacks=[early_stopping], verbose=0)
model_forward.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])
history_forward_sgd = model_forward.fit(X_train, y_train, validation_data=(X_test, y_test),
                                       epochs=100, batch_size=32, callbacks=[early_stopping], verbose=0)

# Backward Propagation Model
model_backward = Sequential([
    Dense(8, activation='relu', input_shape=(8,)),
    Dense(4, activation='relu'),
    Dense(1, activation='sigmoid')
])
model_backward.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history_backward_adam = model_backward.fit(X_train, y_train, validation_data=(X_test, y_test),
                                          epochs=100, batch_size=32, callbacks=[early_stopping], verbose=0)
model_backward.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])
history_backward_sgd = model_backward.fit(X_train, y_train, validation_data=(X_test, y_test),
                                         epochs=100, batch_size=32, callbacks=[early_stopping], verbose=0)

# Wider Model
model_wider = Sequential([
    Dense(32, activation='relu', input_shape=(8,)),
    Dense(16, activation='relu'),
    Dense(1, activation='sigmoid')
])
model_wider.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history_wider_adam = model_wider.fit(X_train, y_train, validation_data=(X_test, y_test),
                                     epochs=100, batch_size=32, callbacks=[early_stopping], verbose=0)
model_wider.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])
history_wider_sgd = model_wider.fit(X_train, y_train, validation_data=(X_test, y_test),
                                    epochs=100, batch_size=32, callbacks=[early_stopping], verbose=0)

# Deeper Model
model_deeper = Sequential([
    Dense(8, activation='relu', input_shape=(8,)),
    Dense(8, activation='relu'),
    Dense(6, activation='relu'),
    Dense(4, activation='relu'),
    Dense(2, activation='relu'),
    Dense(1, activation='sigmoid')
])
model_deeper.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history_deeper_adam = model_deeper.fit(X_train, y_train, validation_data=(X_test, y_test),
                                       epochs=100, batch_size=32, callbacks=[early_stopping], verbose=0)
model_deeper.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])
history_deeper_sgd = model_deeper.fit(X_train, y_train, validation_data=(X_test, y_test),
                                      epochs=100, batch_size=32, callbacks=[early_stopping], verbose=0)

# Plot validation loss for all models
plt.figure(figsize=(10, 6))
plt.plot(history_forward_adam.history['val_loss'], label='Forward (Adam)', linestyle='-', color='blue')
plt.plot(history_forward_sgd.history['val_loss'], label='Forward (SGD)', linestyle='--', color='blue')
plt.plot(history_backward_adam.history['val_loss'], label='Backward (Adam)', linestyle='-', color='green')
plt.plot(history_backward_sgd.history['val_loss'], label='Backward (SGD)', linestyle='--', color='green')
plt.plot(history_wider_adam.history['val_loss'], label='Wider (Adam)', linestyle='-', color='red')
plt.plot(history_wider_sgd.history['val_loss'], label='Wider (SGD)', linestyle='--', color='red')
plt.plot(history_deeper_adam.history['val_loss'], label='Deeper (Adam)', linestyle='-', color='purple')
plt.plot(history_deeper_sgd.history['val_loss'], label='Deeper (SGD)', linestyle='--', color='purple')
plt.title('Validation Loss Comparison Across Models')
plt.xlabel('Epoch')
plt.ylabel('Validation Loss')
plt.legend()
plt.grid(True)
plt.savefig('loss_curves.pdf')
plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import matplotlib.pyplot as plt

# Load dataset
data = pd.read_csv('/content/diabetes.csv')

# Replace zeros with NaN for specific columns
for col in ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']:
    data[col].replace(0, np.nan, inplace=True)
    data[col].fillna(data[col].mean(), inplace=True)

# Define features and target
X = data.drop('Outcome', axis=1)
y = data['Outcome']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Forward Propagation Model (Overfitting)
model_forward_overfit = Sequential([
    Dense(8, activation='relu', input_shape=(8,)),
    Dense(4, activation='relu'),
    Dense(1, activation='sigmoid')
])
model_forward_overfit.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history_forward_overfit = model_forward_overfit.fit(X_train, y_train, validation_data=(X_test, y_test),
                                                   epochs=500, batch_size=32, verbose=0)

# Backward Propagation Model (Overfitting)
model_backward_overfit = Sequential([
    Dense(8, activation='relu', input_shape=(8,)),
    Dense(4, activation='relu'),
    Dense(1, activation='sigmoid')
])
model_backward_overfit.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history_backward_overfit = model_backward_overfit.fit(X_train, y_train, validation_data=(X_test, y_test),
                                                     epochs=500, batch_size=32, verbose=0)

# Wider Model (Overfitting)
model_wider_overfit = Sequential([
    Dense(32, activation='relu', input_shape=(8,)),
    Dense(16, activation='relu'),
    Dense(1, activation='sigmoid')
])
model_wider_overfit.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history_wider_overfit = model_wider_overfit.fit(X_train, y_train, validation_data=(X_test, y_test),
                                               epochs=500, batch_size=32, verbose=0)

# Deeper Model (Overfitting)
model_deeper_overfit = Sequential([
    Dense(8, activation='relu', input_shape=(8,)),
    Dense(8, activation='relu'),
    Dense(6, activation='relu'),
    Dense(4, activation='relu'),
    Dense(2, activation='relu'),
    Dense(1, activation='sigmoid')
])
model_deeper_overfit.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history_deeper_overfit = model_deeper_overfit.fit(X_train, y_train, validation_data=(X_test, y_test),
                                                 epochs=500, batch_size=32, verbose=0)

# Plot validation loss for overfitting analysis
plt.figure(figsize=(10, 6))
plt.plot(history_forward_overfit.history['val_loss'], label='Forward (Overfit)', color='blue')
plt.plot(history_backward_overfit.history['val_loss'], label='Backward (Overfit)', color='green')
plt.plot(history_wider_overfit.history['val_loss'], label='Wider (Overfit)', color='red')
plt.plot(history_deeper_overfit.history['val_loss'], label='Deeper (Overfit)', color='purple')
plt.title('Validation Loss for Overfitting Analysis Across Models')
plt.xlabel('Epoch')
plt.ylabel('Validation Loss')
plt.legend()
plt.grid(True)
plt.savefig('overfit_graphs.pdf')
plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt

# Load dataset
data = pd.read_csv('/content/diabetes.csv')

# Replace zeros with NaN for specific columns
for col in ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']:
    data[col].replace(0, np.nan, inplace=True)
    data[col].fillna(data[col].mean(), inplace=True)

# Define features and target
X = data.drop('Outcome', axis=1)
y = data['Outcome']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Initialize results dictionary
results = {
    'Model': [],
    'Optimizer': [],
    'Accuracy': [],
    'Precision': [],
    'Recall': [],
    'F1-Score': []
}

# Function to evaluate model and store metrics
def evaluate_model(model, X_test, y_test, model_name, optimizer_name, history):
    y_pred = (model.predict(X_test) > 0.5).astype(int)
    results['Model'].append(model_name)
    results['Optimizer'].append(optimizer_name)
    results['Accuracy'].append(accuracy_score(y_test, y_pred))
    results['Precision'].append(precision_score(y_test, y_pred))
    results['Recall'].append(recall_score(y_test, y_pred))
    results['F1-Score'].append(f1_score(y_test, y_pred))
    return history.history['val_accuracy']

# Forward Propagation Model
model_forward = Sequential([
    Dense(8, activation='relu', input_shape=(8,)),
    Dense(4, activation='relu'),
    Dense(1, activation='sigmoid')
])
model_forward.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history_forward_adam = model_forward.fit(X_train, y_train, validation_data=(X_test, y_test),
                                        epochs=100, batch_size=32, callbacks=[early_stopping], verbose=0)
val_acc_forward_adam = evaluate_model(model_forward, X_test, y_test, 'Forward', 'Adam', history_forward_adam)
model_forward.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])
history_forward_sgd = model_forward.fit(X_train, y_train, validation_data=(X_test, y_test),
                                       epochs=100, batch_size=32, callbacks=[early_stopping], verbose=0)
val_acc_forward_sgd = evaluate_model(model_forward, X_test, y_test, 'Forward', 'SGD', history_forward_sgd)

# Backward Propagation Model
model_backward = Sequential([
    Dense(8, activation='relu', input_shape=(8,)),
    Dense(4, activation='relu'),
    Dense(1, activation='sigmoid')
])
model_backward.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history_backward_adam = model_backward.fit(X_train, y_train, validation_data=(X_test, y_test),
                                          epochs=100, batch_size=32, callbacks=[early_stopping], verbose=0)
val_acc_backward_adam = evaluate_model(model_backward, X_test, y_test, 'Backward', 'Adam', history_backward_adam)
model_backward.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])
history_backward_sgd = model_backward.fit(X_train, y_train, validation_data=(X_test, y_test),
                                         epochs=100, batch_size=32, callbacks=[early_stopping], verbose=0)
val_acc_backward_sgd = evaluate_model(model_backward, X_test, y_test, 'Backward', 'SGD', history_backward_sgd)

# Wider Model
model_wider = Sequential([
    Dense(32, activation='relu', input_shape=(8,)),
    Dense(16, activation='relu'),
    Dense(1, activation='sigmoid')
])
model_wider.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history_wider_adam = model_wider.fit(X_train, y_train, validation_data=(X_test, y_test),
                                     epochs=100, batch_size=32, callbacks=[early_stopping], verbose=0)
val_acc_wider_adam = evaluate_model(model_wider, X_test, y_test, 'Wider', 'Adam', history_wider_adam)
model_wider.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])
history_wider_sgd = model_wider.fit(X_train, y_train, validation_data=(X_test, y_test),
                                    epochs=100, batch_size=32, callbacks=[early_stopping], verbose=0)
val_acc_wider_sgd = evaluate_model(model_wider, X_test, y_test, 'Wider', 'SGD', history_wider_sgd)

# Deeper Model
model_deeper = Sequential([
    Dense(8, activation='relu', input_shape=(8,)),
    Dense(8, activation='relu'),
    Dense(6, activation='relu'),
    Dense(4, activation='relu'),
    Dense(2, activation='relu'),
    Dense(1, activation='sigmoid')
])
model_deeper.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
history_deeper_adam = model_deeper.fit(X_train, y_train, validation_data=(X_test, y_test),
                                       epochs=100, batch_size=32, callbacks=[early_stopping], verbose=0)
val_acc_deeper_adam = evaluate_model(model_deeper, X_test, y_test, 'Deeper', 'Adam', history_deeper_adam)
model_deeper.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])
history_deeper_sgd = model_deeper.fit(X_train, y_train, validation_data=(X_test, y_test),
                                      epochs=100, batch_size=32, callbacks=[early_stopping], verbose=0)
val_acc_deeper_sgd = evaluate_model(model_deeper, X_test, y_test, 'Deeper', 'SGD', history_deeper_sgd)

# Create results table
results_df = pd.DataFrame(results)

# Round numerical values for cleaner display
results_df['Accuracy'] = results_df['Accuracy'].round(4)
results_df['Precision'] = results_df['Precision'].round(4)
results_df['Recall'] = results_df['Recall'].round(4)
results_df['F1-Score'] = results_df['F1-Score'].round(4)

# Save results as LaTeX table
latex_table = results_df.to_latex(index=False, float_format="%.4f", caption="Performance Comparison of All Models", label="tab:model_comparison")
with open('model_comparison_table.tex', 'w') as f:
    f.write(latex_table)

# Print results table
print("\nModel Performance Comparison:")
print(results_df)

# Determine best model based on F1-Score
best_model = results_df.loc[results_df['F1-Score'].idxmax()]
print("\nBest Model Decision:")
print(f"Model: {best_model['Model']} with {best_model['Optimizer']} optimizer")
print(f"F1-Score: {best_model['F1-Score']:.4f}, Accuracy: {best_model['Accuracy']:.4f}, "
      f"Precision: {best_model['Precision']:.4f}, Recall: {best_model['Recall']:.4f}")

# Plot validation accuracy for all models
plt.figure(figsize=(10, 6))
plt.plot(val_acc_forward_adam, label='Forward (Adam)', linestyle='-', color='blue')
plt.plot(val_acc_forward_sgd, label='Forward (SGD)', linestyle='--', color='blue')
plt.plot(val_acc_backward_adam, label='Backward (Adam)', linestyle='-', color='green')
plt.plot(val_acc_backward_sgd, label='Backward (SGD)', linestyle='--', color='green')
plt.plot(val_acc_wider_adam, label='Wider (Adam)', linestyle='-', color='red')
plt.plot(val_acc_wider_sgd, label='Wider (SGD)', linestyle='--', color='red')
plt.plot(val_acc_deeper_adam, label='Deeper (Adam)', linestyle='-', color='purple')
plt.plot(val_acc_deeper_sgd, label='Deeper (SGD)', linestyle='--', color='purple')
plt.title('Validation Accuracy Comparison Across Models')
plt.xlabel('Epoch')
plt.ylabel('Validation Accuracy')
plt.legend()
plt.grid(True)
plt.savefig('model_comparison.pdf')
plt.show()